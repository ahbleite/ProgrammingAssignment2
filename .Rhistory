load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
library(tidyr)
tweets <- trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource, "source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library(lubridate)
library(scales)
library(ggplot2)
library(stringr)
tweets %>%
count(source, hour = hour(with_tz(created, "EST"))) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)",
y = "% of tweets",
color = "")
tweet_picture_counts <- tweets %>%
filter(!str_detect(text, '^"')) %>%
count(source,
picture = ifelse(str_detect(text, "t.co"),
"Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
library(tidytext)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
#tweet_words
android_iphone_ratios <- tweet_words %>%
count(word, source) %>%
filter(sum(n) >= 5) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
mutate(logratio = log2(Android / iPhone)) %>%
arrange(desc(logratio))
nrc <- sentiments %>%
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment)
#nrc
sources <- tweet_words %>%
group_by(source) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(id, source, total_words)
by_source_sentiment <- tweet_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, id) %>%
ungroup() %>%
complete(sentiment, id, fill = list(n = 0)) %>%
inner_join(sources) %>%
group_by(source, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
head(by_source_sentiment)
install.packages(broom)
install.packages('broom'')
install.packages('broom')
install.packages("broom")
library(dplyr)
library(purrr)
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
library(tidyr)
tweets <- trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource, "source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library(lubridate)
library(scales)
library(ggplot2)
library(stringr)
tweets %>%
count(source, hour = hour(with_tz(created, "EST"))) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)",
y = "% of tweets",
color = "")
tweet_picture_counts <- tweets %>%
filter(!str_detect(text, '^"')) %>%
count(source,
picture = ifelse(str_detect(text, "t.co"),
"Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
library(tidytext)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
#tweet_words
android_iphone_ratios <- tweet_words %>%
count(word, source) %>%
filter(sum(n) >= 5) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
mutate(logratio = log2(Android / iPhone)) %>%
arrange(desc(logratio))
nrc <- sentiments %>%
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment)
#nrc
sources <- tweet_words %>%
group_by(source) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(id, source, total_words)
by_source_sentiment <- tweet_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, id) %>%
ungroup() %>%
complete(sentiment, id, fill = list(n = 0)) %>%
inner_join(sources) %>%
group_by(source, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
#head(by_source_sentiment)
library(broom)
sentiment_differences <- by_source_sentiment %>%
group_by(sentiment) %>%
do(tidy(poisson.test(.$words, .$total_words)))
sentiment_differences
rbinom(10, 10, 0.6)
formals(rbinom)
args(lm)
args(rbinom)
?lm
?aov
args(paste)
args(cat)
?cat
filter(lexicon == "nrc") %>%
x[1]
rm(list=ls())
ls
ls()
all.names()
all.names("")
all.names()
mean
?mean
?'%>%'
?'>'
?':'
?'%&%'
?complete.cases
x<-1:100
x
filter1 <- x[1][x>31]
filter1
filter1 <- x[x>31]
filter1
rm(filter1)
ls
ls()
y<-101:200
length(x)
length(y)
z<-combine(x,y)
z<-ccomb(x,y)
z<-cbind(x,y)
z
z[1][z>31]
z[z[1]>31,2]
z[z>31,2]
z[z>31]
z[1][z>31]
z[1][z>31]
z[2][z>31]
z[1]
z[2]
z[1]
z[1][1]
z[1][5]
z[1][2]
z[2][1]
z[6][1]
z[6][2]
z[6][5]
z[6][2]
z[100][1]
z[101][1]
z
z[101]
z[2000]
z[200]
z[201]
z[200,]
z[1,4]
df<-data.frame(z)
df
df[df$x>31&df$y>160,]
rm(list-lm())
rm(list=lm())
rm(list=lm())
lc()
ls()
rm(list=ls())
clean()
make.power <-function(n)
{
pow <- function(x)
{
x^n
}
po
}
x<-3
make.power(2)
make.power <-function(n)
{
pow <- function(x)
{
x^n
}
pow
}
make.power(2)
x
cube<-make.power(3)
cube
rm(list=ls())
}
make.power <-function(n){
pow <- function(x)  {
x^n
}
pow
}
x<-3
cube<-make.power(3)
cube
cube(3)
square<-make.power(2)
square(2)
ls(environment(cube)
get("n", environment(cube))
ls(environment(square)
get("n", environment(square))
ls(environment(cube))
get("n", environment(cube))
ls(environment(square))
get("n", environment(square))
y<-10
f<-function(x)
{
y<-2
y^2 + g(x)
}
g<-function(x)
{
x*y
}
f(3)
g<-function(x)
{
a<-3
x+a+y
}
g(2)
y<-3
g(2)
rm(list=ls())
g<-function(x)
{
a<-3
x+a+y
}
g(2)
y<-3
g(2)
make.NegLogLik <- function(data, fixed=c(FALSE,FALSE))
{
params <- fixed
function(p)
{
params[!fixed] <- params
mu <- params[1]
sigma <- params[2]
a <- -0.5*length(data)*log(2*pi*sigma^2)
b <- -0.5*sum((data-mu)^2)/(sigma^2)
-(a+b)
}
}
rm(list=ls())
set.seed(1); normals<-rnorm(100,1,2)
nLL <- make.NegLogLik(normals)
nLL
ls(environment(nLL))
make.NegLogLik <- function(data, fixed=c(FALSE,FALSE))
{
params <- fixed
function(p)
{
params[!fixed] <- params
mu <- params[1]
sigma <- params[2]
a <- -0.5*length(data)*log(2*pi*sigma^2)
b <- -0.5*sum((data-mu)^2)/(sigma^2)
-(a+b)
}
}
set.seed(1); normals<-rnorm(100,1,2)
nLL <- make.NegLogLik(normals)
nLL
ls(environment(nLL))
nLL
nLL(c(FALSE,FALSE)
)
optim(c(mu=0, sigma=1), nLL)$params
optim(c(mu=0, sigma=1), nLL)$par
nLL <- make.NegLogLik(normals, c(FALSE, 2))
optimize(nLL, c(-1,3))$minimum
warnings()
nLL <- make.NegLogLik(normals, c(1,False))
optimize(nLL, c(1e-6,10))$minimum
rm(list=ls())
make.NegLogLik <- function(data, fixed=c(FALSE,FALSE))
{
params <- fixed
function(p)
{
params[!fixed] <- p
mu <- params[1]
sigma <- params[2]
a <- -0.5*length(data)*log(2*pi*sigma^2)
b <- -0.5*sum((data-mu)^2)/(sigma^2)
-(a+b)
}
}
set.seed(1); normals<-rnorm(100,1,2)
nLL <- make.NegLogLik(normals)
nLL
ls(environment(nLL))
optim(c(mu=0, sigma=1), nLL)$par
nLL <- make.NegLogLik(normals, c(FALSE, 2))
optimize(nLL, c(-1,3))$minimum
nLL <- make.NegLogLik(normals, c(1,False))
optimize(nLL, c(1e-6,10))$minimum
nLL <- make.NegLogLik(normals, c(1,FALSE))
optimize(nLL, c(1e-6,10))$minimum
nLL <- make.NegLogLik(normals, c(1, FALSE))
x <- seq(1.7, 1.9, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "l")
nLL <- make.NegLogLik(normals, c(FALSE, 2))
x <- seq(0.5, 1.5, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "l")
x
y
?plot
nLL <- make.NegLogLik(normals, c(FALSE, 2))
x <- seq(0.5, 1.5, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "p")
?plot
nLL <- make.NegLogLik(normals, c(FALSE, 2))
x <- seq(0.5, 1.5, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "h")
?plot
nLL <- make.NegLogLik(normals, c(FALSE, 2))
x <- seq(0.5, 1.5, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "o")
x<-as.Date("1970-01-01")
x
unclass(x)
unclass(x)
unclass(as.Date("1970-01-02")
)
Sys.time()
x<-Sys.time()
p<-POXIXlt(x)
names(unclass(p))
x<-Sys.time()
p<-POSIXlt(x)
names(unclass(p))
x<-Sys.time()
p<-as.POSIXlt(x)
names(unclass(p))
p$sec
p$isdst
unclass(x)
?strptime
unclass(x)
class(x)
x
names(unclass(x))
names(unclass(p))
z<-as.POSIXlt(x)
z-p
z<z+10
z
z-p
rm(list=ls())
matrix()
x<-matrix(1:8,4,4)
x
x<-matrix(1:16,4,4)
x
y<-solve(x)
rm(list=ls())
x<-matrix(c(2,2,3,2),2,2)
x
y<-solve(x)
y
x<-matrix(c(4,3,3,2),2,2)
y<-solve(x)
y
setwd("~/RWorkingDirectory")
source("makeVector.R")
makeVector()
class(makeVector())
?'<<-'
y <-function() x
x<-12
y
y()
makeVector()
makeVector()$set(10)
makeVector()$m
makeVector()$getmean()
makeVector()$setmean()
makeVector()$setmean(mean())
vect <- rnorm(n = 1000)
vect
makeVector()$set(vect)
makeVector()$set()
makeVector()$get()
makeVector()$set(vect)
makeVector()$get()
makeVector()$getmean()
makeVector()$set(vect)
source("cacheman.R")
source("cachemean.R")
rm(list=ls())
source("makeVector.R")
source("cachemean.R")
vect <- rnorm(n = 1000)
cachemean(makeVector()$set(vect)
)
cachemean(makeVector()$set(vect))
x <- makeVector(vect)
x
x$get()
x$getmean()
cachemean(x)
cachemean(x)
vect <- c("a","b", "c")
x <- makeVector(vect)
x
x$get()
cachemean(x)
rm(list=ls())
setwd("~/RProgAssig")
setwd("~/RProgAssig/ProgrammingAssignment2")
source("cachematrix.R")
my_matrix <- matrix(c("4,3,32"),2,2)
class(my_matrix)
source("cachematrix.R")
cacheSolve(my_data)
cacheSolve(my_matrix)
class(my_matrix)
class(my_matrix)
my_matrix
rm(list=ls())
source("cachematrix.R")
my_matrix <- makeCa( matrix(c("4,3,32"),2,2))
my_matrix <- makeCacheMatrix( matrix(c("4,3,32"),2,2))
my_matrix
cacheSolve(my_matrix)
my_matrix <- makeCacheMatrix( matrix(c(4,3,3,2),2,2))
cacheSolve(my_matrix)
my_matrix
cacheSolve(my_matrix)
?solve
my_matrix$get()
cacheSolve(my_matrix)
debug(cacheSolve)
cacheSolve(my_matrix)
undebug(cacheSolve)
bye
Q
cacheSolve(my_matrix)
source("cachematrix.R")
cacheSolve(my_matrix)
source("cachematrix.R")
cacheSolve(my_matrix)
cacheSolve(my_matrix)
source("cachematrix.R")
cacheSolve(my_matrix)
rm(list=ls())
source("cachematrix.R")
my_matrix <- makeCacheMatrix( matrix(c(4,3,3,2),2,2))
cacheSolve(my_matrix)
